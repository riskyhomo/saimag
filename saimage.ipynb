{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcwh4G_lEHe5"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers == 0.11.1\n",
        "!pip install transformers scipy ftfy accelerate"
      ],
      "metadata": {
        "id": "vM8A7qVMEwZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe= StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\" , troch_dtype= torch.float16)"
      ],
      "metadata": {
        "id": "uj_7VTsBLRbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "D8H8pjCaNM5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"a boy is riding a car \"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image.save(f'trial.png')\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "qgSLVjjANTih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "generator = torch.Generator(\"cuda\").manual_seed(42)\n",
        "image= pipe(prompt, num_inference_steps=50, generator= generator).images[0]\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "G9WJNnEvO6-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "1NtTqNGjQQY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "  assert len(imgs)== rows*cols\n",
        "\n",
        "  w,h = imgs[0].size\n",
        "  grid = Image.new('RGB',size=(cols*w,rows*h))\n",
        "  grid_w,grid_h= grid_size\n",
        "\n",
        "\n",
        "  for i, img in enumerate(imgs):\n",
        "    grid.paste(img, box=(i%cols*w, i//colsh))\n",
        "\n",
        "  return grid"
      ],
      "metadata": {
        "id": "k7FRPAfWQ3B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "num_cols=4\n",
        "num_rows=4\n",
        "\n",
        "prompt =[\"girls are playing in park\"] *num_cols\n",
        "images = pipe(prompt).images\n",
        "\n",
        "all_images=[]\n",
        "for i in range(num_rows):\n",
        "  images(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "\n",
        "grid= image_grid(all_images,rows= num_rows, cols=num_cols)\n",
        "\n",
        "grid\n"
      ],
      "metadata": {
        "id": "p7LAhN-FT0sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "num_cols = 2\n",
        "num_rows = 3\n",
        "\n",
        "prompt = [\"girls are playing in park\"] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ],
      "metadata": {
        "id": "n3ZykfsZR1Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch_device = 'cuda'if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "y_bXIqjuWkI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "#Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "\n",
        "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "#  The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5dZpvKV_W-tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\",subfolder = \"scheduler\")"
      ],
      "metadata": {
        "id": "KfzeWOZOZqAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#moving to GPU\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ],
      "metadata": {
        "id": "n1Ny1-jBaC9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=['a girl is sitting in a park']\n",
        "height = 512\n",
        "width = 512\n",
        "\n",
        "num_inference_steps= 100\n",
        "guidance_scale = 8\n",
        "generator =torch.manual_seed(42)\n",
        "batch_size=1"
      ],
      "metadata": {
        "id": "VBDYIfGnaGh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "RpbCwlfUasxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "7M2yFtb-a3eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ],
      "metadata": {
        "id": "MyGa9vP8a-WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)"
      ],
      "metadata": {
        "id": "QqfGBW_ibB9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents.shape"
      ],
      "metadata": {
        "id": "VpybKixjbGic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(num_inference_steps)\n",
        "latents= latents*scheduler.init_noise_sigma"
      ],
      "metadata": {
        "id": "hKKPzfyLbLqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ],
      "metadata": {
        "id": "2Qvttf8_bcmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample"
      ],
      "metadata": {
        "id": "PFPGMuQYboue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "5AytCQllb587"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}